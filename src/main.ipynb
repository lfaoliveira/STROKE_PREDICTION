{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b20c91",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e86314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Define paths based on environment\n",
    "if Path(\"/kaggle\").exists():\n",
    "    os.environ[\"AMBIENTE\"] = \"KAGGLE\"\n",
    "    os.environ[\"TENSORBOARD_NO_TF\"] = \"1\"\n",
    "\n",
    "    PATH_DATASET = Path(\"/kaggle/working/STROKE_PREDICTION\")\n",
    "    PATH_CODE = PATH_DATASET / \"src\"\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET\n",
    "elif Path(\"/content\").exists():\n",
    "    os.environ[\"AMBIENTE\"] = \"COLAB\"\n",
    "    PATH_DATASET = Path(\"/content/DELETAR\")\n",
    "    PATH_CODE = PATH_DATASET / \"src\"\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET / \"outputs\"\n",
    "else:\n",
    "    os.environ[\"AMBIENTE\"] = \"LOCAL\"\n",
    "    PATH_CODE = Path.cwd()\n",
    "    PATH_DATASET = PATH_CODE.parent\n",
    "    PATH_OUTPUT_DIR = PATH_DATASET / \"outputs\"\n",
    "\n",
    "\n",
    "# Check if installation has been done\n",
    "INSTALL_MARKER = PATH_DATASET / \".install_complete\"\n",
    "try:\n",
    "    if not INSTALL_MARKER.exists():\n",
    "        # Install uv\n",
    "        pass\n",
    "        !pip install uv\n",
    "\n",
    "        # Environment-specific setup\n",
    "        if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "            import kaggle_secrets\n",
    "\n",
    "            user_secrets = kaggle_secrets.UserSecretsClient()\n",
    "            github_pat = user_secrets.get_secret(\"GITHUB_PAT\")\n",
    "\n",
    "            os.chdir(\"/kaggle/working\")\n",
    "            os.system(\n",
    "                f\"git clone https://{github_pat}@github.com/lfaoliveira/STROKE_PREDICTION.git\"\n",
    "            )\n",
    "            os.chdir(PATH_DATASET)\n",
    "\n",
    "        elif os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "            os.system(\"git pull origin main\")\n",
    "\n",
    "        # Install dependencies\n",
    "        os.chdir(PATH_DATASET)\n",
    "        os.system(\"uv pip install --requirements pyproject.toml --system\")\n",
    "\n",
    "        if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "            os.system(\n",
    "                \"uv pip install --upgrade --force-reinstall --no-cache-dir scipy numpy matplotlib protobuf tensorboard\"\n",
    "            )\n",
    "\n",
    "        # Mark installation as complete\n",
    "        INSTALL_MARKER.touch()\n",
    "        print(\"Installation completed\")\n",
    "    else:\n",
    "        print(\"Installation already completed, skipping...\")\n",
    "\n",
    "    os.chdir(PATH_CODE)\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "except Exception:\n",
    "    print(\"FALHA AO INICIAR NOTEBOOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226e767",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataProcesser.dataset import StrokeDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "df = StrokeDataset().original_df.copy()\n",
    "\n",
    "# =========================\n",
    "# 1. Dataset overview\n",
    "# =========================\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\\n\", df.isna().sum())\n",
    "\n",
    "# =========================\n",
    "# 2. Target distribution\n",
    "# =========================\n",
    "stroke_dist = df[\"stroke\"].value_counts(normalize=True) * 100\n",
    "print(\"\\nStroke distribution (%):\\n\", stroke_dist)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# =========================\n",
    "# 1. Stroke distribution\n",
    "# =========================\n",
    "df[\"stroke\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[0])\n",
    "axes[0].set_title(\"Stroke Distribution (Imbalanced)\")\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels([\"No Stroke\", \"Stroke\"], rotation=0)\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_yticks([i for i in range(0, 4501, 500)])\n",
    "# =========================\n",
    "# 2. Age vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"age\", by=\"stroke\", ax=axes[1])\n",
    "axes[1].set_title(\"Age by Stroke\")\n",
    "axes[1].set_xlabel(\"Stroke\")\n",
    "axes[1].set_ylabel(\"Age\")\n",
    "\n",
    "# =========================\n",
    "# 3. Glucose vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"avg_glucose_level\", by=\"stroke\", ax=axes[2])\n",
    "axes[2].set_title(\"Avg Glucose by Stroke\")\n",
    "axes[2].set_xlabel(\"Stroke\")\n",
    "axes[2].set_ylabel(\"Glucose Level\")\n",
    "\n",
    "# =========================\n",
    "# 4. BMI vs Stroke\n",
    "# =========================\n",
    "df.boxplot(column=\"bmi\", by=\"stroke\", ax=axes[3])\n",
    "axes[3].set_title(\"BMI by Stroke\")\n",
    "axes[3].set_xlabel(\"Stroke\")\n",
    "axes[3].set_ylabel(\"BMI\")\n",
    "\n",
    "# =========================\n",
    "# 5. Hypertension\n",
    "# =========================\n",
    "pd.crosstab(df[\"hypertension\"], df[\"stroke\"], normalize=\"index\").mul(100).plot(\n",
    "    kind=\"bar\", ax=axes[4], rot=0\n",
    ")\n",
    "axes[4].set_title(\"Stroke Rate by Hypertension\")\n",
    "axes[4].set_ylabel(\"Percentage (%)\")\n",
    "axes[4].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 6. Heart disease\n",
    "# =========================\n",
    "pd.crosstab(df[\"heart_disease\"], df[\"stroke\"], normalize=\"index\").mul(100).plot(\n",
    "    kind=\"bar\", ax=axes[5], rot=0\n",
    ")\n",
    "axes[5].set_title(\"Stroke Rate by Heart Disease\")\n",
    "axes[5].set_ylabel(\"Percentage (%)\")\n",
    "axes[5].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 7. Smoking status\n",
    "# =========================\n",
    "pd.crosstab(df[\"smoking_status\"], df[\"stroke\"], normalize=\"index\").mul(100).sort_values(\n",
    "    by=1\n",
    ").plot(kind=\"barh\", ax=axes[6])\n",
    "axes[6].set_title(\"Stroke Rate by Smoking Status\")\n",
    "axes[6].set_xlabel(\"Percentage (%)\")\n",
    "axes[6].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 8. Work type\n",
    "# =========================\n",
    "pd.crosstab(df[\"work_type\"], df[\"stroke\"], normalize=\"index\").mul(100).sort_values(\n",
    "    by=1\n",
    ").plot(kind=\"bar\", ax=axes[7], rot=30)\n",
    "axes[7].set_title(\"Stroke Rate by Work Type\")\n",
    "axes[7].set_ylabel(\"Percentage (%)\")\n",
    "axes[7].legend([\"No Stroke\", \"Stroke\"])\n",
    "\n",
    "# =========================\n",
    "# 9. Empty / summary slot\n",
    "# =========================\n",
    "axes[8].axis(\"off\")\n",
    "axes[8].set_title(\"EDA Summary Slot\")\n",
    "\n",
    "plt.suptitle(\"Stroke Dataset – Exploratory Data Analysis\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef61c38",
   "metadata": {},
   "source": [
    "## Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222276af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import gc\n",
    "import mlflow\n",
    "import torch\n",
    "from Models.mlp import MLP, MLPSearchSpace\n",
    "from Models.kan import MyKan, KANSearchSpace\n",
    "from lightning import seed_everything, Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from mlflow.pytorch import autolog\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from DataProcesser.datamodule import StrokeDataModule\n",
    "\n",
    "\n",
    "def zip_res(\n",
    "    path_sqlite: str, path_mlflow: Path, filename: str, dest_folder: Path | None = None\n",
    "):\n",
    "    import shutil\n",
    "\n",
    "    path_sqlite_clean = path_sqlite.replace(\"sqlite:///\", \"\")\n",
    "    print(f\"CWD: {Path.cwd()}\\n\")\n",
    "    PATH_TEMP = Path.cwd() / \"ZIP_TEMP\"\n",
    "    shutil.rmtree(PATH_TEMP, ignore_errors=True)\n",
    "    PATH_TEMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(path_sqlite_clean, PATH_TEMP / Path(path_sqlite_clean).name)\n",
    "    shutil.copytree(path_mlflow, PATH_TEMP / path_mlflow.name)\n",
    "\n",
    "    # Determine destination folder\n",
    "    if dest_folder is None:\n",
    "        dest_folder = Path.cwd()\n",
    "    else:\n",
    "        dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create zip file in destination folder\n",
    "    zip_path = dest_folder / filename.replace(\".zip\", \"\")\n",
    "    shutil.make_archive(str(zip_path), \"zip\", PATH_TEMP)\n",
    "    shutil.rmtree(PATH_TEMP)\n",
    "    print(f\"PATH ZIPFILE: {zip_path.with_suffix('.zip').resolve()}\")\n",
    "\n",
    "\n",
    "def model_choice(CHOICE, INPUT_DIMS, N_CLASSES):\n",
    "    if CHOICE == \"MLP\":\n",
    "        search_space = MLPSearchSpace()\n",
    "\n",
    "        keys = search_space.Keys\n",
    "        hyperparams = {\n",
    "            keys.BATCH_SIZE: 32,\n",
    "            keys.HIDDEN_DIMS: 512,\n",
    "            keys.LR: 1e-3,\n",
    "            keys.WEIGHT_DECAY: 1e-5,\n",
    "            keys.BETA0: 0.900,\n",
    "            keys.BETA1: 0.99,\n",
    "            keys.N_LAYERS: 24,\n",
    "        }\n",
    "        suggested_hparams = search_space.suggest(hyperparams)\n",
    "        model = MLP(\n",
    "            INPUT_DIMS, N_CLASSES, recall_factor=4.0, hyperparameters=suggested_hparams\n",
    "        )\n",
    "    elif CHOICE == \"KAN\":\n",
    "        search_space = KANSearchSpace()\n",
    "        keys = search_space.Keys\n",
    "        hyperparams = {\n",
    "            keys.BATCH_SIZE: 32,\n",
    "            keys.HIDDEN_DIMS: 16,\n",
    "            keys.LR: 1e-3,\n",
    "            keys.WEIGHT_DECAY: 1e-5,\n",
    "            keys.BETA0: 0.900,\n",
    "            keys.BETA1: 0.99,\n",
    "            keys.GRID: 24,\n",
    "            keys.SPLINE_POL_ORDER: 3,\n",
    "        }\n",
    "        suggested_hparams = search_space.suggest(hyperparams)\n",
    "        model = MyKan(\n",
    "            INPUT_DIMS, N_CLASSES, recall_factor=4.0, hyperparameters=suggested_hparams\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"ESCOLHA DE MODELO ERRADA!\")\n",
    "    return model, suggested_hparams, keys\n",
    "\n",
    "\n",
    "## -----------------------------COLAR NO KAGGLE------------------\n",
    "def main(CHOICE: str):\n",
    "    ###------SEEDS---------###\n",
    "    RAND_SEED = 42\n",
    "    seed_everything(RAND_SEED)\n",
    "    AMBIENTE = os.environ[\"AMBIENTE\"]\n",
    "    GPU = True if AMBIENTE in [\"KAGGLE\", \"COLAB\"] else False\n",
    "    ## ----------VARIAVEIS TREINO-----------\n",
    "    cpus = os.cpu_count()\n",
    "    WORKERS = cpus if cpus is not None else 1\n",
    "    NUM_DEVICES = 1 if GPU else 1\n",
    "    NUM_NODES = 1\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 2\n",
    "    PATIENCE = 20\n",
    "    ARTIFACT_PATH = PATH_OUTPUT_DIR / \"artifacts\"\n",
    "    os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "    #### -------- VARIAVEIS DE LOGGING ------------\n",
    "    EXP_NAME = f\"stroke_{CHOICE}_1\"\n",
    "    RUN_NAME: str | None = f\"normal_{CHOICE}\"\n",
    "    MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "\n",
    "    mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "    mlflow.set_experiment(EXP_NAME)\n",
    "    autolog(log_models=True, checkpoint=True, exclusive=False)\n",
    "\n",
    "    ## ----------VARIAVEIS MODELO-----------\n",
    "    N_CLASSES = 2\n",
    "\n",
    "    datamodule = StrokeDataModule(BATCH_SIZE, WORKERS)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    INPUT_DIMS = datamodule.input_dims or -1\n",
    "    assert INPUT_DIMS > 0\n",
    "    model, hparams, keys = model_choice(CHOICE, INPUT_DIMS, N_CLASSES)\n",
    "\n",
    "    _ = model(model.example_input_array)\n",
    "\n",
    "    # loop principal de treinamento\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        active_run_id = run.info.run_id\n",
    "\n",
    "        mlflow_logger = MLFlowLogger(\n",
    "            experiment_name=EXP_NAME,\n",
    "            tracking_uri=MLF_TRACK_URI,\n",
    "            log_model=True,\n",
    "            run_id=active_run_id,\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=PATIENCE, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=EPOCHS,\n",
    "            devices=NUM_DEVICES,\n",
    "            accelerator=\"gpu\" if GPU else \"cpu\",\n",
    "            num_nodes=NUM_NODES,\n",
    "            logger=mlflow_logger,\n",
    "            enable_checkpointing=False,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        mlflow.log_params(dict(model.hparams))\n",
    "\n",
    "        # Test and log artifacts (useful for the Analysis section)\n",
    "        test_df = datamodule.dataset.original_df.copy()\n",
    "        test_df[\"pred\"] = None\n",
    "        test_df[\"error\"] = None\n",
    "\n",
    "        test_loader, test_dataset = datamodule.test_dataloader()\n",
    "        for batch_idx, test_batch in enumerate(test_loader):\n",
    "            ret_dict = model.test_step(\n",
    "                batch=test_batch,\n",
    "                batch_idx=batch_idx,\n",
    "                output_df=test_df,\n",
    "                test_dataset=test_dataset,\n",
    "            )\n",
    "            test_df = ret_dict[\"output_df\"]\n",
    "\n",
    "        name = f\"test_results_{run.info.run_id}.csv\"\n",
    "        path_test_csv = Path(ARTIFACT_PATH, name)\n",
    "        test_df.to_csv(path_test_csv)\n",
    "        mlflow.log_artifact(str(path_test_csv))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    NAME_RESZIP = f\"resultado_kaggle_{EXP_NAME}\"\n",
    "    MLRUNS_FOLDER = Path.cwd() / \"mlruns\"\n",
    "    zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP)\n",
    "    print(\"\\n\", \"=\" * 60)\n",
    "    print(f\"RESULTADOS ZIPADOS {Path(NAME_RESZIP).resolve()}\")\n",
    "    print(\"=\" * 60, \"\\n\")\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        ARQ_TYPE = Literal[\"MLP\", \"KAN\", \"SVM\", \"XGBOOST\"]  ## MODEL ARCHITECTURE\n",
    "        models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "        for choice in models:\n",
    "            # trains model based on architecture\n",
    "            main(choice)\n",
    "\n",
    "        NAME_RESZIP = \"resultado_kaggle_stroke_normal\"\n",
    "        MLRUNS_FOLDER = Path.cwd() / \"mlruns\"\n",
    "        MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "        ZIP_ROOT = (\n",
    "            PATH_DATASET / \"..\" if os.environ[\"AMBIENTE\"] == \"KAGGLE\" else PATH_DATASET\n",
    "        )\n",
    "        zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP, ZIP_ROOT)\n",
    "        print(\"\\n\", \"=\" * 60)\n",
    "        print(f\"RESULTADOS ZIPADOS {Path(ZIP_ROOT, NAME_RESZIP).resolve()}\")\n",
    "        print(\"=\" * 60, \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    gc.collect()\n",
    "\n",
    "    if os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "        from view.dashboard import see_model\n",
    "\n",
    "        see_model(PATH_DATASET / \"mlflow.db\", PATH_DATASET / \"..\" / \"mlruns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528cdda",
   "metadata": {},
   "source": [
    "## Training with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from typing import Literal\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from Models.mlp import MLP, MLPSearchSpace\n",
    "from lightning import Callback, seed_everything, Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from mlflow.pytorch import autolog\n",
    "from DataProcesser.datamodule import StrokeDataModule\n",
    "import optuna\n",
    "from Models.kan import KANSearchSpace, MyKan\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def zip_res(\n",
    "    path_sqlite: str, path_mlflow: Path, filename: str, dest_folder: Path | None = None\n",
    "):\n",
    "    import shutil\n",
    "\n",
    "    path_sqlite_clean = path_sqlite.replace(\"sqlite:///\", \"\")\n",
    "    print(f\"CWD: {Path.cwd()}\\n\")\n",
    "    PATH_TEMP = Path.cwd() / \"ZIP_TEMP\"\n",
    "    shutil.rmtree(PATH_TEMP, ignore_errors=True)\n",
    "    PATH_TEMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy(path_sqlite_clean, PATH_TEMP / Path(path_sqlite_clean).name)\n",
    "    shutil.copytree(path_mlflow, PATH_TEMP / path_mlflow.name)\n",
    "\n",
    "    # Determine destination folder\n",
    "    if dest_folder is None:\n",
    "        dest_folder = Path.cwd()\n",
    "    else:\n",
    "        dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create zip file in destination folder\n",
    "    zip_path = dest_folder / filename.replace(\".zip\", \"\")\n",
    "    shutil.make_archive(str(zip_path), \"zip\", PATH_TEMP)\n",
    "    shutil.rmtree(PATH_TEMP)\n",
    "    print(f\"PATH ZIPFILE: {zip_path.with_suffix('.zip').resolve()}\")\n",
    "\n",
    "\n",
    "def supress_warnings():\n",
    "    import logging\n",
    "\n",
    "    # Suppress specific MLflow warnings\n",
    "    logging.getLogger(\"mlflow.utils.requirements_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress PyTorch Lightning info messages\n",
    "    logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Suppress Optuna info messages\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def model_choice(\n",
    "    CHOICE, INPUT_DIMS, trial: optuna.trial.Trial, N_CLASSES, hyperparameters=None\n",
    "):\n",
    "    if CHOICE == \"MLP\":\n",
    "        search_space = MLPSearchSpace()\n",
    "        suggested_hparams = search_space.suggest_optuna(trial)\n",
    "        keys = search_space.Keys\n",
    "        model = MLP(INPUT_DIMS, N_CLASSES,recall_factor=1.8, hyperparameters=suggested_hparams)\n",
    "    elif CHOICE == \"KAN\":\n",
    "        search_space = KANSearchSpace()\n",
    "        suggested_hparams = search_space.suggest_optuna(trial)\n",
    "        keys = search_space.Keys\n",
    "        model = MyKan(INPUT_DIMS, N_CLASSES,recall_factor=1.8, hyperparameters=suggested_hparams)\n",
    "    else:\n",
    "        raise ValueError(\"ESCOLHA DE MODELO ERRADA!\")\n",
    "    return model, suggested_hparams, keys\n",
    "\n",
    "\n",
    "## -----------------------------COLAR NO KAGGLE------------------\n",
    "def main(CHOICE: str, MLF_TRACK_URI: str):\n",
    "    ###------SEEDS---------###\n",
    "    RAND_SEED = 42\n",
    "    seed_everything(RAND_SEED)\n",
    "    AMBIENTE = os.environ[\"AMBIENTE\"]\n",
    "    GPU = True if AMBIENTE in [\"KAGGLE\", \"COLAB\"] else False\n",
    "    ## ----------VARIAVEIS TREINO-----------\n",
    "    cpus = os.cpu_count()\n",
    "    WORKERS = cpus if cpus is not None else 1\n",
    "    NUM_DEVICES = 1 if GPU else 1\n",
    "    NUM_NODES = 1\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 2\n",
    "    TRIALS = 1\n",
    "    PATIENCE = 25\n",
    "    EARLY_STOP = True\n",
    "    PRINT_MODEL_SUMMARY = False\n",
    "    ARTIFACT_PATH = PATH_OUTPUT_DIR / \"artifacts\"\n",
    "    os.makedirs(ARTIFACT_PATH, exist_ok=True)\n",
    "\n",
    "    #### -------- VARIAVEIS DE LOGGING ------------\n",
    "    EXP_NAME = f\"stroke_{CHOICE}_1\"\n",
    "    RUN_NAME: str | None = None  # nome da RUN: pode ser aleatório ou definido\n",
    "\n",
    "    mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "    mlflow.set_experiment(EXP_NAME)\n",
    "    autolog(log_models=True, checkpoint=True, exclusive=False)\n",
    "\n",
    "    ## ----------VARIAVEIS MODELO-----------\n",
    "    N_CLASSES = 2\n",
    "\n",
    "    datamodule = StrokeDataModule(BATCH_SIZE, WORKERS)\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    INPUT_DIMS = datamodule.input_dims or -1\n",
    "    assert INPUT_DIMS > 0\n",
    "\n",
    "    # Progress bar for trials\n",
    "    pbar = tqdm(\n",
    "        total=TRIALS,\n",
    "        desc=f\"Optuna Trials ({CHOICE})\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        colour=\"green\",\n",
    "    )\n",
    "\n",
    "    # loop principal de treinamento\n",
    "    def objective(trial: optuna.Trial):\n",
    "\n",
    "        model, hyperparameters, keys = model_choice(\n",
    "            CHOICE,\n",
    "            INPUT_DIMS,\n",
    "            trial,\n",
    "            N_CLASSES,\n",
    "        )\n",
    "\n",
    "        batch_size = hyperparameters[keys.BATCH_SIZE]\n",
    "\n",
    "        # Recreate dataloaders with trial batch_size\n",
    "        train_loader, val_loader = (\n",
    "            datamodule.train_dataloader(batch_size),\n",
    "            datamodule.val_dataloader(batch_size),\n",
    "        )\n",
    "        # model hyperparameters\n",
    "        hyperparameters = None\n",
    "\n",
    "        _ = model(model.example_input_array)\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True) as run:\n",
    "            active_run_id = run.info.run_id\n",
    "\n",
    "            mlflow_logger = MLFlowLogger(\n",
    "                experiment_name=EXP_NAME,\n",
    "                tracking_uri=MLF_TRACK_URI,\n",
    "                log_model=True,\n",
    "                run_id=active_run_id,\n",
    "            )\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=PATIENCE, mode=\"min\"\n",
    "            )\n",
    "            callbacks: list[Callback] | None = [early_stopping] if EARLY_STOP else None\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=EPOCHS,\n",
    "                devices=NUM_DEVICES,\n",
    "                accelerator=\"gpu\" if GPU else \"cpu\",\n",
    "                num_nodes=NUM_NODES,\n",
    "                logger=mlflow_logger,\n",
    "                enable_checkpointing=False,  # must be disabled for mlflow correct logging\n",
    "                enable_model_summary=PRINT_MODEL_SUMMARY,\n",
    "                enable_progress_bar=False,  # Disable PyTorch Lightning progress bar, to avoid log polution\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "\n",
    "            trainer.fit(\n",
    "                model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "            )\n",
    "            mlflow.log_params(dict(model.hparams))\n",
    "            # mlflow_log_model(model, artifact_path=\"model\")\n",
    "            val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "            test_df = datamodule.dataset.original_df.copy()\n",
    "            test_df[\"pred\"] = None\n",
    "            test_df[\"error\"] = None\n",
    "\n",
    "            test_loader, test_dataset = datamodule.test_dataloader(batch_size)\n",
    "            for batch_idx, test_batch in enumerate(test_loader):\n",
    "                test_df = model.test_step(\n",
    "                    batch=test_batch,\n",
    "                    batch_idx=batch_idx,\n",
    "                    output_df=test_df,\n",
    "                    test_dataset=test_dataset,\n",
    "                )\n",
    "\n",
    "            name = f\"test_results_{run.info.run_id}.csv\"\n",
    "            path_test_csv = Path(ARTIFACT_PATH, name)\n",
    "            # Ensure no directory exists with the same name before saving the file\n",
    "            if path_test_csv.exists() and path_test_csv.is_dir():\n",
    "                import shutil\n",
    "                shutil.rmtree(path_test_csv)\n",
    "            \n",
    "            test_df.to_csv(path_test_csv)\n",
    "            assert test_df is not None and path_test_csv.exists()\n",
    "            mlflow.log_artifact(str(path_test_csv))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"val_loss\": f\"{val_loss:.4f}\", \"trial\": trial.number})\n",
    "            return val_loss\n",
    "\n",
    "    with mlflow.start_run(run_name=RUN_NAME) as parent_run:\n",
    "        study = optuna.create_study(direction=\"minimize\", study_name=f\"{CHOICE}\")\n",
    "        study.optimize(objective, n_trials=TRIALS, gc_after_trial=True)\n",
    "\n",
    "        # Log best parameters\n",
    "        mlflow.log_params(\n",
    "            {\"best_\" + k: v for k, v in study.best_trial.params.items()},\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\n",
    "            \"best_val_loss\",\n",
    "            study.best_trial.value or float(\"inf\"),\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "        print(\"Best validation loss:\", study.best_trial.value)\n",
    "        best_run_id = study.best_trial.user_attrs.get(\"run_id\")\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\"best_trial_id\": best_run_id},\n",
    "            run_id=parent_run.info.run_id,\n",
    "        )\n",
    "\n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "\n",
    "        # Identify and tag the best run (no rename_run in MLflow Python API)\n",
    "        experiment = mlflow.get_experiment_by_name(EXP_NAME)\n",
    "        if experiment:\n",
    "            runs_df = pd.DataFrame(\n",
    "                mlflow.search_runs(\n",
    "                    experiment_ids=[experiment.experiment_id],\n",
    "                    order_by=[\"metrics.val_loss ASC\"],\n",
    "                )\n",
    "            )\n",
    "            runs_df = runs_df.dropna(subset=[\"metrics.val_loss\"])\n",
    "            if not runs_df.empty:\n",
    "                best_run_id = runs_df.iloc[0].run_id\n",
    "                prefix = os.environ[\"OPTUNA_BEST_RUN_PREFIX\"]\n",
    "                mlflow.MlflowClient().set_tag(\n",
    "                    best_run_id, \"mlflow.runName\", f\"{prefix}_{CHOICE}\"\n",
    "                )\n",
    "            else:\n",
    "                raise ModuleNotFoundError(\"Runs Dataframe empty\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        ARQ_TYPE = Literal[\n",
    "            \"MLP\", \"KAN\", \"SVM\", \"XGBOOST\", \"RNDFOREST\", \"LIQUIDNN\"\n",
    "        ]  ## MODEL ARCHITECTURE\n",
    "        models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "        MLF_TRACK_URI = f\"sqlite:///{PATH_CODE}/mlflow.db\"\n",
    "\n",
    "        os.environ[\"OPTUNA\"] = \"True\"\n",
    "        os.environ[\"OPTUNA_BEST_RUN_PREFIX\"] = \"best_run\"\n",
    "\n",
    "        supress_warnings()\n",
    "        for i, choice in enumerate(models):\n",
    "            # trains model based on architecture\n",
    "            clear_output(wait=True)\n",
    "            main(choice, MLF_TRACK_URI)\n",
    "\n",
    "        NAME_RESZIP = \"resultado_kaggle\"\n",
    "        MLRUNS_FOLDER = PATH_CODE / \"mlruns\"\n",
    "        ZIP_ROOT = (\n",
    "            PATH_DATASET / \"..\" if os.environ[\"AMBIENTE\"] == \"KAGGLE\" else PATH_DATASET\n",
    "        )\n",
    "        zip_res(MLF_TRACK_URI, MLRUNS_FOLDER, NAME_RESZIP, ZIP_ROOT)\n",
    "        print(\"\\n\", \"=\" * 60)\n",
    "        print(f\"RESULTADOS ZIPADOS {Path(ZIP_ROOT, NAME_RESZIP).resolve()}\")\n",
    "        print(\"=\" * 60, \"\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"PREMATURELY INTERRUPTING...\\n\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    gc.collect()\n",
    "\n",
    "    if os.environ[\"AMBIENTE\"] == \"LOCAL\":\n",
    "        from view.dashboard import see_model\n",
    "\n",
    "        see_model(PATH_DATASET / \"mlflow.db\", PATH_DATASET / \"..\" / \"mlruns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb81704",
   "metadata": {},
   "source": [
    "## Results Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from typing import Literal\n",
    "from DataProcesser.utils import final_analysis\n",
    "\n",
    "# Saves directly to env output dir\n",
    "output_dir = PATH_OUTPUT_DIR\n",
    "if os.environ[\"AMBIENTE\"] == \"KAGGLE\":\n",
    "    output_dir = PATH_DATASET.parent\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{PATH_CODE}/mlflow.db\")\n",
    "ARQ_TYPE = Literal[\"MLP\", \"KAN\", \"SVM\", \"XGBOOST\"]  ## MODEL ARCHITECTURE\n",
    "models: list[ARQ_TYPE] = [\"MLP\", \"KAN\"]\n",
    "os.environ[\"OPTUNA\"] = \"True\"\n",
    "\n",
    "SORT_METRIC = \"val_f_beta_avg\"\n",
    "RESIDUAL = True\n",
    "compare_df, resProcesser = final_analysis(\n",
    "    models, output_dir, SORT_METRIC, residual=RESIDUAL\n",
    ")\n",
    "\n",
    "compared_df = compare_df.drop(columns=\"epoch_avg\")\n",
    "compare_df.to_csv(output_dir / \"classify_results.csv\")\n",
    "print(compare_df.to_string())\n",
    "\n",
    "ser_fbeta = compare_df[SORT_METRIC].sort_values(ascending=False)\n",
    "best_model, fbeta_value = next(ser_fbeta.items())\n",
    "print(f\"BEST MODEL: *{best_model}* WITH F-BETA: {fbeta_value}\\n\")\n",
    "\n",
    "# If true, execute residual analysis of best model's errors\n",
    "if RESIDUAL:\n",
    "    # resProcesser.fit_predict(str(best_model))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c39d6",
   "metadata": {},
   "source": [
    "## MLFlow's Dashboard (Only works outside of Kaggle)\n",
    "### Download the training results from Kaggle and paste them into a cloned folder of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1cfda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def see_model(database: pathlib.Path, folder: pathlib.Path):\n",
    "    subprocess.Popen(\n",
    "        [\n",
    "            \"mlflow\",\n",
    "            \"ui\",\n",
    "            \"--backend-store-uri\",\n",
    "            f\"sqlite:///{database}\",\n",
    "            \"--default-artifact-root\",\n",
    "            folder,\n",
    "            \"--host\",\n",
    "            \"127.0.0.1\",\n",
    "            \"--port\",\n",
    "            \"5000\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PATH_RES_ZIPADO = Path(\n",
    "        \"C:\\\\Users\\\\LUIS_FELIPE\\\\Downloads\\\\resultado_kaggle_stroke_1.zip\"\n",
    "    )\n",
    "    DIR = Path(Path.cwd(), PATH_RES_ZIPADO.name.replace(\".zip\", \"\"))\n",
    "    print(f\"DIR: {DIR}\")\n",
    "    if DIR.exists():\n",
    "        shutil.rmtree(DIR)\n",
    "    DIR.mkdir()\n",
    "    shutil.unpack_archive(PATH_RES_ZIPADO, DIR)\n",
    "\n",
    "    print(\"COMECANDO SUBPROCESSO!\\n\")\n",
    "    see_model(DIR / \"mlflow.db\", DIR / \"mlruns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto-kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
